{
    "id": "fc5393437f",
    "level": "sentence",
    "abstract": [],
    "body_text": [
        {
            "text": "are important in the toolkits of applied econometricians and statisticians, and they are likely to be more important in the future. "
        },
        {
            "text": "We will focus our intuitive discussion on deriving more accurate confidence bands in ordinary least squares regressions, the most commonly used model in applied econometrics. "
        },
        {
            "text": "However, computationally intensive approaches also can be used to remove biases in estimated coefficients and can be applied to more general models. "
        },
        {
            "text": "Indeed, these techniques may be most appealing or powerful when applied in more complex settings as an alternative to analytical estimates."
        },
        {
            "text": "The theoretical justification for the bootstrap, multiple imputations and standard analytic inference methods commonly used in applied econometrics is based on large-sample approximations. "
        },
        {
            "text": "In the remainder of this paper, we will use the formal term \"consistent\" to mean that as the sample size of the estimation data set increases, the estimator gets closer to its true value in the underlying population."
        },
        {
            "text": "To motivate the discussion of the bootstrap approach in regression analysis, begin by considering the simpler problem of estimating the sampling distribution (for example, standard errors, confidence intervals and other statistical measures) of the mean height among a country's population, based on a small random sample. ",
            "section": "The Bootstrap"
        },
        {
            "text": "The sample mean typically will differ from the population mean due to sampling error, and the sampling distribution summarizes how the sample mean will vary across a large number of independent samples from the same population. ",
            "section": "The Bootstrap"
        },
        {
            "text": "Efron's (1979Efron's ( , 1982 bootstrap method treats the observed sample as the population and draws samples from this approximate population to estimate the sampling distribution. ",
            "section": "The Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 13,
                    "type": "bibr",
                    "text": "Efron's (1979"
                },
                {
                    "start": 13,
                    "end": 29,
                    "type": "bibr",
                    "ref_id": "b4",
                    "text": "Efron's ( , 1982"
                }
            ]
        },
        {
            "text": "These bootstrap samples are drawn with replacement from the original observed data, and the mean is computed for each bootstrap sample. ",
            "section": "The Bootstrap"
        },
        {
            "text": "This produces a data set of estimated means, with size equal to the number of bootstrap repetitions. ",
            "section": "The Bootstrap"
        },
        {
            "text": "The distribution of these resampled means is used to approximate the sampling distribution of the population mean.",
            "section": "The Bootstrap"
        },
        {
            "text": "This bootstrap method described above will only give accurate estimates if the original sample is large enough to reflect the true population accurately. ",
            "section": "The Bootstrap"
        },
        {
            "text": "The traditional analytic approach approximates the sampling distribution by a normal distribution centered at the sample mean with variance equal to the sample variance. ",
            "section": "The Bootstrap"
        },
        {
            "text": "This traditional approximation requires that the sample be large enough for the Central Limit Theorem to apply to the sample mean. ",
            "section": "The Bootstrap"
        },
        {
            "text": "If the sample size is small and the true population is not normally distributed, then the bootstrap approximation should be more accurate.",
            "section": "The Bootstrap"
        },
        {
            "text": "The application of the bootstrap to linear regression models is similar to the sample mean case above, although several different approaches are possible. ",
            "section": "The Bootstrap"
        },
        {
            "text": "The most popular way to implement the bootstrap to estimate the sampling distribution of the least squares coefficient estimators is the XY, or paired bootstrap. ",
            "section": "The Bootstrap"
        },
        {
            "text": "Using this approach, with an observed sample of size N, randomly draw N complete data points with replacement from the observed sample (or, in other words, sample rows of the data matrix). ",
            "section": "The Bootstrap"
        },
        {
            "text": "Then recompute the least squares estimator for each bootstrap sample. ",
            "section": "The Bootstrap"
        },
        {
            "text": "This produces a different estimate of the coefficients for each repetition. ",
            "section": "The Bootstrap"
        },
        {
            "text": "The distribution of the resulting set of estimated coefficients is an estimate of their sampling distribution. ",
            "section": "The Bootstrap"
        },
        {
            "text": "Whether the error terms are homoskedastic or heteroskedastic, the paired bootstrap will generate consistent estimates of the sampling distribution of the least squares estimator. ",
            "section": "The Bootstrap"
        },
        {
            "text": "Brownstone and Kazimi (2000), Efron and Tibshirani (1993), Horowitz (2001) and Jeong and Maddala (1993) provide more complete applied discussions of this and other bootstrap procedures, including details regarding hypothesis tests and confidence intervals based on the bootstrapped estimates. ",
            "section": "The Bootstrap",
            "ref_spans": [
                {
                    "start": 30,
                    "end": 57,
                    "type": "bibr",
                    "text": "Efron and Tibshirani (1993)"
                },
                {
                    "start": 59,
                    "end": 74,
                    "type": "bibr",
                    "text": "Horowitz (2001)"
                }
            ]
        },
        {
            "text": "1 Horowitz (1997) points out that the different nature of the bootstrap approximation provides a rough check on the adequacy of large-sample theory. ",
            "section": "The Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b19",
                    "text": "1"
                }
            ]
        },
        {
            "text": "If the assumptions commonly made for large-sample econometrics hold-for example, residuals that are independent and distributed identically with finite variancethen the bootstrap and standard methods used in econometric software packages should yield the same answers. ",
            "section": "The Bootstrap"
        },
        {
            "text": "If significant departures from these assumptions are present, then one or both of the methods are not reliable for the particular problem and data. ",
            "section": "The Bootstrap"
        },
        {
            "text": "Deciding between them requires more detailed analysis, which explains why much of the ongoing research on the bootstrap is based on simulations rather than on empirical data sets.",
            "section": "The Bootstrap"
        },
        {
            "text": "Despite this inherent uncertainty about the superiority of the bootstrap in specific applications, Hall (1992) has shown that in many cases the bootstrap achieves accurate estimates of sampling distributions at smaller sample sizes than standard large-sample analytic techniques. ",
            "section": "The Bootstrap"
        },
        {
            "text": "This finding holds for most of the statistics that are commonly used for testing hypotheses about parameter estimates from econometric models, such as t-statistics and others based on standard normal and chi-square distributions. ",
            "section": "The Bootstrap"
        },
        {
            "text": "2 This feature of the bootstrap provides the strongest argument for its widespread application in common econometric models, although the simple bootstrap methods described in this paper need to be modified to simulate the distribution of test statistics rather than coefficient estimates for Hall's results to apply.",
            "section": "The Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b20",
                    "text": "2"
                }
            ]
        },
        {
            "text": "We illustrate these ideas with an example based on a standard regression equation for explaining male earnings. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The dependent variable is the log of yearly earnings, deflated by the GDP deflator for personal consumption expenditure. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We report coefficient estimates for the following explanatory variables: years of formal education, potential labor market experience (and its square), and tenure at the current firm, along with dummy variables indicating whether the respondent is a union member, black, married and resides in a metropolitan statistical area (MSA). ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We also included a constant and dummy variables representing region of residence and observation years. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The data are drawn from the Panel Study of Income Dynamics (PSID) for the years 1984 -1993. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We restricted the sample to male household heads, aged 21-64 and employed in the private sector (excluding self-employed individuals). ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We report results based on a 2 percent random sample of individuals observed at any time in the panel (260 observations on 50 individuals), since in large samples all the procedures give the same results. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Using a small subsample also allows us to simulate the true sampling distribution by drawing many independent subsamples of 50 individuals from the data and reestimating the model for each subsample. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "This simulation uses the original PSID sample of 2,789 individuals (16,327 panel observations) to approximate the true population of anybody who could have been sampled by the PSID. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Table 1 presents results for the male wage equation. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 7,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "The estimated regression coefficients are listed in the second column. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The next column gives the true confidence intervals calculated from 1000 independent subsamples of 50 individuals from the data. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "These are the true confidence intervals if the PSID main sample is identical to the population. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "If the PSID main sample is only an approximation of the true population, then these \"true\" confidence intervals are bootstrap intervals with bootstrap sample size much smaller than the data. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "It is, of course, more efficient to use the entire sample in this case, but we are only interested in using these intervals as a benchmark for comparing results based on only one subsample.",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The subsequent four columns in Table 1 list 95 percent confidence intervals estimated in different ways from the original subsample of 260 observations. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 31,
                    "end": 38,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "The first column of confidence intervals shows those obtained through ordinary least squares regression analysis. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The next column provides confidence intervals based on the \"robust\" estimator of the model's sampling variance, which was developed independently by Huber (1967) and White (1980White ( , 1982. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 149,
                    "end": 161,
                    "type": "bibr",
                    "ref_id": "b5",
                    "text": "Huber (1967)"
                },
                {
                    "start": 166,
                    "end": 177,
                    "type": "bibr",
                    "text": "White (1980"
                },
                {
                    "start": 177,
                    "end": 191,
                    "type": "bibr",
                    "ref_id": "b16",
                    "text": "White ( , 1982"
                }
            ]
        },
        {
            "text": "The robust estimator of sampling variance does not require homoskedasticity-that the errors in the regression model have constant variance. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Instead, the Huber-White estimator approximates the variance of each residual by the square of the least squares residual. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We use a generalization of the Huber-White estimator that also allows for the residuals to be correlated across time for a given individual. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "In the example in Table 1, the confidence intervals from the robust regression are noticeably wider than the ordinary least squares confidence intervals, which suggests that heteroskedasticity or a similar departure from the common assumptions used in applying ordinary least squares is present.",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 18,
                    "end": 25,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "The third set of confidence intervals is obtained through application of the paired (XY ) bootstrap procedure. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "For each bootstrap repetition, we sample randomly with replacement from the set of 50 individuals in the original sample. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We take all of the dependent and independent variables for all years for the sampled individual and continue until a bootstrap sample of approximately the same size as the empirical data set has been selected. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The regression equation is then reestimated for each bootstrap sample. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The coefficient estimates from each repetition are saved in a file, and the resulting data set (whose size is determined by the number of coefficients and bootstrap repetitions) provides an estimate of the sampling distribution of the estimated coefficients.",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "All of the bootstrapped distributions reported in the table were obtained using 1000 bootstrap repetitions. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Although standard errors can be reliably estimated with fewer repetitions (on the order of 100), confidence intervals directly based on the bootstrap distribution will be more accurate than those based on standard errors if the bootstrap distribution is skewed. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Once the repetitions were completed, we obtained the 95 percent confidence intervals through the simple \"percentile method\"-by setting them equal to the values corresponding to the 2.5 and 97.5 percentiles of the bootstrap distributions of the estimated coefficients. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "3  For the final column, we used a different bootstrap approach due to Wu (1986), known as the \"wild bootstrap.\" ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b21",
                    "text": "3"
                },
                {
                    "start": 71,
                    "end": 80,
                    "type": "bibr",
                    "ref_id": "b17",
                    "text": "Wu (1986)"
                }
            ]
        },
        {
            "text": "First, run an ordinary least squares regression, and save the residuals. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Then multiply each value of the residual by a random variable that takes on one of two values: ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "i) (1 \u03ea \u034c 5)/2 with probability (1 \u03e9 \u034c 5)/(2 \u034c 5); or ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "ii) (1 \u03e9 \u034c 5)/2 with probability 1 \u03ea (1 \u03e9 \u034c 5)/(2 \u034c 5). ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Note that this random variable has a mean of zero with variance equal to one. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The resulting bootstrap residuals therefore will have mean zero and variance equal to the square of the least squares residual for that observation, which is identical to the approximation used to calculate Huber-White robust standard errors. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The resampled residual vector is then added to the predicted value from the original regression to create the resampled dependent variable. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "We modified the wild bootstrap to account for the panel data structure of our data by multiplying the entire vector of residuals for each individual by the same draw of the two-point random variable defined above. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The wild bootstrap confidence intervals reported in the table are based on the calculation of t-statistics using the Huber-White robust standard errors for each bootstrap sample. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "This method should be superior to simple percentile method intervals in small samples (for details, see Brownstone and Kazimi, 2000; Horowitz, 2001). ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "Both paired bootstrapping and wild bootstrapping will provide consistent inference with heteroskedasticity, but wild bootstrapping generally will be more accurate, since, unlike the paired bootstrap, it imposes a restriction on each bootstrap sample that is identical to the conditions used to identify the ordinary least squares model. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "4 The least squares confidence intervals in Table 1 are much narrower than the true bands. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b22",
                    "text": "4"
                },
                {
                    "start": 44,
                    "end": 51,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "The other confidence intervals are usually narrower than the true bands, but not for all of the coefficients. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The confidence intervals based on the paired bootstrap are somewhat wider than the robust confidence intervals. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "In contrast, the wild bootstrap confidence intervals are narrower than the paired bootstrap intervals and are close to the robust intervals in width. ",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The bootstrap intervals are closer to the truth for six of the eight coefficients in Table 1, although the theoretical superiority of the wild bootstrap bands is not clear from this example. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 85,
                    "end": 92,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "Of course, the results in Table 1 are just based on one subsample from the PSID, and the differences between the bootstrap and robust regression intervals are not large. ",
            "section": "An Empirical Example: An Earnings Regression",
            "ref_spans": [
                {
                    "start": 26,
                    "end": 33,
                    "type": "table",
                    "ref_id": "tab_0",
                    "text": "Table 1"
                }
            ]
        },
        {
            "text": "Most of the studies finding large differences between the bootstrap and standard methods have used nonlinear models and more complex estimators (Horowitz, 1997; 2001).",
            "section": "An Empirical Example: An Earnings Regression"
        },
        {
            "text": "The discussion of the bootstrap to this point has focused on a linear regression model with error terms that are independent, but not necessarily normally distrib-uted or homoskedastic. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "However, there has been considerable work in recent years applying the bootstrap approach to other situations.",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Cross-section extensions are numerous. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Any of the bootstrapping methods discussed above for the linear regression model can be applied to the nonlinear regression model, with the caveat that bootstrapped residuals must be adjusted to insure that their mean is zero. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "In the standard analytic approach, obtaining the sampling distribution of nonlinear regression estimates relies on a linear approximation to the regression function. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Because bootstrap methods evaluate the appropriate transformations by simulation rather than approximation, we would expect bootstrap estimates of the sampling distribution to be more accurate than the standard analytic estimates as the linear approximation becomes less accurate. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Horowitz (2001, section 5.3) verified this in a small simulation study of a particular nonlinear regression model. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "5 In time series models, error terms are often serially correlated, and this requires bootstrap approaches that differ from the cross-section case. ",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b23",
                    "text": "5"
                }
            ]
        },
        {
            "text": "The simplest approach, called model-based bootstrapping, is to assume that the time series can be fit by an appropriate autoregressive moving average model with serially uncorrelated residuals. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Then the bootstrap can be implemented using bootstrap approaches that take samples from the residuals of a regression (as discussed in more detail in footnote 1), although the method of generating the bootstrap samples from these bootstrapped residuals needs to be modified to account for the dynamic nature of the time series model. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "6 Similarly, in panel data settings, the bootstrap must be modified to account for time dependence for observations within crosssection units-for example, through bootstrap sampling of the complete set of observations for cross-section units (Ziliak, 1997).",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b24",
                    "text": "6"
                },
                {
                    "start": 242,
                    "end": 256,
                    "type": "bibr",
                    "ref_id": "b18",
                    "text": "(Ziliak, 1997)"
                }
            ]
        },
        {
            "text": "Although it is mechanically possible to apply the bootstrap to any model, there are circumstances where the bootstrap will fail to estimate the sampling distribution consistently (for details, see Horowitz, 2001, section 2.1). ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "For example, Andrews (2000) shows that the bootstrap does not accurately estimate the sampling distribution of a coefficient whose true value is zero and whose estimate is constrained to be greater than or equal to zero.",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 13,
                    "end": 27,
                    "type": "bibr",
                    "ref_id": "b0",
                    "text": "Andrews (2000)"
                }
            ]
        },
        {
            "text": "Despite these caveats and inherent uncertainty about the superiority of the bootstrap in specific settings, the availability of powerful desktop computers makes widespread use of the bootstrap very tempting. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Bootstrap estimates-especially based on the paired approach-are trivial to obtain once the estimation model is specified. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "The statistical package Stata has several bootstrap procedures and subroutines available in its core program, and a researcher who has never read a single 5 Other useful cross-section applications, surveyed in Brownstone and Kazimi (2000), include quantile regression, discrete choice and hazard models, and frontier production function estimates. ",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 155,
                    "end": 156,
                    "type": "bibr",
                    "text": "5"
                }
            ],
            "entity_spans": [
                {
                    "start": 24,
                    "end": 29,
                    "type": "software",
                    "rawForm": "Stata",
                    "resp": "#annotator13",
                    "id": "fc5393437f-software-simple-0",
                    "cert": "1.0"
                }
            ]
        },
        {
            "text": "6 Bickel and Freedman (1983) provided an early example of model-based bootstrapping for time series. ",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "text": "6"
                },
                {
                    "start": 2,
                    "end": 28,
                    "type": "bibr",
                    "text": "Bickel and Freedman (1983)"
                }
            ]
        },
        {
            "text": "Their approach has been generalized to autoregressive moving average models (Berkowitz and Kilian, 2000; Li and Maddala, 1996) and cointegrated regression models (Li and Maddala, 1997). ",
            "section": "Extensions and Limitations of the Bootstrap",
            "ref_spans": [
                {
                    "start": 91,
                    "end": 126,
                    "type": "bibr",
                    "text": "Kilian, 2000; Li and Maddala, 1996)"
                },
                {
                    "start": 162,
                    "end": 184,
                    "type": "bibr",
                    "text": "(Li and Maddala, 1997)"
                }
            ]
        },
        {
            "text": "Kilian (1998a, b) considers bootstrapping vector autoregression models and applies this technique to examine international effects of monetary policy. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "word about the bootstrap could produce bootstrapped estimates in Stata with little difficulty. ",
            "section": "Extensions and Limitations of the Bootstrap",
            "entity_spans": [
                {
                    "start": 65,
                    "end": 70,
                    "type": "software",
                    "rawForm": "Stata",
                    "resp": "#annotator13",
                    "id": "fc5393437f-software-simple-1",
                    "cert": "0.9"
                }
            ]
        },
        {
            "text": "Researchers should be cautious about such casual application, because results regarding the reliability and accuracy of bootstrapped sampling distributions are not yet available for many classes of econometric models. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "However, while we await further results, the bootstrap can be very useful for simple applications such as estimating the sampling distributions of nonparametric statistics (for example, quantiles and percentiles of univariate distributions) or statistics with illdefined distributions (see Valletta, 1993, footnote 13, for an example).",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "word about the bootstrap could produce bootstrapped estimates in Stata with little difficulty. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Researchers should be cautious about such casual application, because results regarding the reliability and accuracy of bootstrapped sampling distributions are not yet available for many classes of econometric models. ",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "However, while we await further results, the bootstrap can be very useful for simple applications such as estimating the sampling distributions of nonparametric statistics (for example, quantiles and percentiles of univariate distributions) or statistics with illdefined distributions (see Valletta, 1993, footnote 13, for an example).",
            "section": "Extensions and Limitations of the Bootstrap"
        },
        {
            "text": "Applied econometricians routinely face the problem of missing values of key variables in their data. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The multiple imputation technique was developed by Rubin (1987) as a general, computationally intensive method \"to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities\" (Rubin, 1996). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 51,
                    "end": 63,
                    "type": "bibr",
                    "ref_id": "b13",
                    "text": "Rubin (1987)"
                },
                {
                    "start": 252,
                    "end": 265,
                    "type": "bibr",
                    "ref_id": "b13",
                    "text": "(Rubin, 1996)"
                }
            ]
        },
        {
            "text": "This approach represents a useful compromise between the na\u00efve extremes of eliminating observations with missing values from the analysis sample or replacing missing values with a single imputed (fitted) value. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The former approach typically underutilizes the full information available in the data, whereas the latter typically leads to an overstatement of statistical precision in resulting estimates. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The technique also can be used for consistent inference using data that are mismeasured rather than missing.",
            "section": "Multiple Imputations"
        },
        {
            "text": "Consider the general application to a linear regression model. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Multiple imputation consists of two main steps. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "In the first step, the data provider or analyst uses all available data to estimate a proper imputation model, which provides multiple fitted values of the missing or erroneous variables. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "In the second step, the analyst combines the estimated imputations with the main data set and estimates the regression equation of interest.",
            "section": "Multiple Imputations"
        },
        {
            "text": "The full definition of a proper imputation procedure is given in Rubin (1987, pp. 118 -119). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 65,
                    "end": 90,
                    "type": "bibr",
                    "text": "Rubin (1987, pp. 118 -119"
                }
            ]
        },
        {
            "text": "If we have a model for predicting the missing (or erroneous) values conditional on all observed data, then we can use this model to make independent simulated draws for the missing data. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The safest way to implement a proper imputation procedure, as Rubin first proposed, is to draw explicitly from the (Bayesian) posterior predictive distribution of the missing values under a specific model. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "7 However, there are other proper multiple imputation procedures that require no explicit Bayesian calculations. ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b25",
                    "text": "7"
                }
            ]
        },
        {
            "text": "Indeed, although Rubin developed the theoretical properties of this methodology for Bayesian models, he showed that these results also apply in large samples to classical statistical models (Rubin, 1987, chapter 4).",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 190,
                    "end": 214,
                    "type": "bibr",
                    "text": "(Rubin, 1987, chapter 4)"
                }
            ]
        },
        {
            "text": "Whether the prediction approach is Bayesian or non-Bayesian, any proper imputation procedure must condition on all observed data. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "In addition, different sets of imputed values must be drawn independently so that they reflect all sources of uncertainty in the response process.",
            "section": "Multiple Imputations"
        },
        {
            "text": "Once the imputations are provided, the substantive model is estimated once for each set of imputed values. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The resulting estimates are averaged to obtain consistent parameter estimates; depending on the pattern of missing responses, the estimates are likely to differ from those obtained using na\u00efve approaches to missing data. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The covariance matrix of these coefficient estimates is determined by the average of the usual covariance matrix of the coefficient estimates across different sets of imputations, plus a term that represents the variability of the estimates across the different imputed data sets (see Rubin, 1987, or Brownstone andValletta, 1996, for the exact expressions). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 285,
                    "end": 315,
                    "type": "bibr",
                    "text": "Rubin, 1987, or Brownstone and"
                },
                {
                    "start": 315,
                    "end": 329,
                    "type": "bibr",
                    "text": "Valletta, 1996"
                }
            ]
        },
        {
            "text": "This covariance estimator is consistent as long as there are at least two sets of imputed values, but increasing the number of sets of imputed values clearly improves the accuracy of the covariance component due to uncertainty in the imputation process.",
            "section": "Multiple Imputations"
        },
        {
            "text": "The goal is for the imputed data to provide both consistent estimators of coefficients and a measure of the statistical precision or confidence intervals for these coefficients, where the confidence intervals must be broadened somewhat to account for the component of variance associated with imputation error. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "As long as the variables used in the imputation model have an identifiable, systematic relationship with the imputed variables, the imputation process adds information; the precision of this information is limited by the precision of the imputation process. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The computations required typically can be performed using standard econometric packages with relatively minimal programming, although we are not aware of any econometrics packages that currently provide multiple imputation routines or subroutines.",
            "section": "Multiple Imputations"
        },
        {
            "text": "Several statistical agencies have incorporated or are experimenting with multiply imputed values in public-use survey data. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Such applications are a key component of Rubin's original vision for the methodology, which he views as an appropriate form of data enhancement by public agencies whose survey information may be more extensive than that available to end users (Rubin, 1996). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 243,
                    "end": 256,
                    "type": "bibr",
                    "ref_id": "b13",
                    "text": "(Rubin, 1996)"
                }
            ]
        },
        {
            "text": "By exploiting confidential information (such as precise location or financial characteristics) in the imputation procedure, data agencies can better meet the simultaneous demands of providing high-quality data, while protecting survey respondents against breaches of confidentiality. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Moreover, one advantage of multiple imputations is that it can be done once by the data provider and then used for a variety of purposes by many analysts.",
            "section": "Multiple Imputations"
        },
        {
            "text": "Among the first major applications of multiple imputations was a project conducted at the U.S. Census Bureau to assess the comparability of the different industry and occupation codings in the 1970 and 1980 censuses (Clogg et al., 1991). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 216,
                    "end": 236,
                    "type": "bibr",
                    "ref_id": "b2",
                    "text": "(Clogg et al., 1991)"
                }
            ]
        },
        {
            "text": "In this project, analysts used a Bayesian strategy to provide multiply imputed values of 1980 industry and occupation codes in public-use samples from the 1970 census. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The imputations were based on logistic regression models estimated from a special subsample of the 1970 census units for which the 1970 and 1980 codes had been recorded. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The resulting multiply imputed data set has provided a useful bridge for analysts who rely on accurate assessment of trends in the relative prevalence of precise industry and occupation categories.",
            "section": "Multiple Imputations"
        },
        {
            "text": "In probably the most important ongoing application, the Federal Reserve has provided multiply imputed income and wealth variables in the public release of its Survey of Consumer Finances since 1989 (Kennickell, 1998). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 198,
                    "end": 216,
                    "type": "bibr",
                    "text": "(Kennickell, 1998)"
                }
            ]
        },
        {
            "text": "This survey focuses on household finances, a subject for which responses are sensitive and often defined imprecisely. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "As a result, the survey exhibits high rates of missing information. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Fed analysts have developed and implemented a complex iterative imputation procedure for continuous, multinomial and binary variables. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The imputation model relies on the publicly available survey information concerning consumer income and wealth, combined with confidential information such as census tract characteristics and industry and occupation characteristics tabulated from the Current Population Survey of the U.S. Bureau of Labor Statistics. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Although these multiply imputed data have been used successfully by many researchers, Kennickell reports frequent misuse and notes that \"the need for education is great.\" 8 Multiple imputations have been used less in specific applications than in general data provision. ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 171,
                    "end": 172,
                    "type": "bibr",
                    "ref_id": "b26",
                    "text": "8"
                }
            ]
        },
        {
            "text": "However, Brownstone and Valletta (1996) extended the basic approach to the specific application of correcting for measurement error in the dependent variable in earnings regression equations. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "9 Research on earnings data, which compares surveyed earnings responses to validated earnings data gathered through company personnel files or Social Security records, has shown that measurement error in survey earnings is large and correlated with important explanatory variables (for example, Bound et al., 1994; Bound and Krueger, 1991; and Duncan and Hill, 1985). ",
            "section": "Multiple Imputations",
            "ref_spans": [
                {
                    "start": 0,
                    "end": 1,
                    "type": "bibr",
                    "ref_id": "b27",
                    "text": "9"
                },
                {
                    "start": 344,
                    "end": 366,
                    "type": "bibr",
                    "ref_id": "b3",
                    "text": "Duncan and Hill, 1985)"
                }
            ]
        },
        {
            "text": "Thus, measurement error in earnings is likely to bias estimated coefficients and statistical confidence in earnings regression models.",
            "section": "Multiple Imputations"
        },
        {
            "text": "In Brownstone and Valletta (1996), we estimated equations for the determinants of earnings where the explanatory variables included experience, job tenure, union membership and dummy variables for union membership, race, marriage, blue-collar work and whether the job was hourly. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "We used survey data on earnings from the Panel Study of Income Dynamics (PSID) combined with information from the PSID Validation Study (PSIDVS). ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The PSIDVS collected standard PSID variables for several hundred employees from a large Detroit area manufacturing firm in 1983 and 1987 and matched this information with company personnel records on earnings and other variables. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The company earnings records are highly accurate and were treated as error-free in our analyses.",
            "section": "Multiple Imputations"
        },
        {
            "text": "The first step in the multiple imputation process is to estimate an imputation model for company record earnings in the PSIDVS as a function of an error-prone survey earnings variable and the other variables as the explanatory variables. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The estimated imputation model is used to provide multiply imputed estimates of true earnings in the main data set. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "The procedure is completed by combining the multiply imputed estimates of true earnings with observed values of the explanatory variables and using the pooled data for estimating the final earnings equation.",
            "section": "Multiple Imputations"
        },
        {
            "text": "In our analysis of data for income years 1982 and 1986, we found that measurement error biases several coefficients from the earnings equation and often causes underestimation of their sampling variance. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "In both cross-section and longitudinal equations, accounting for measurement error reduces the estimated effect of union membership on earnings and causes the negative effect of working in a blue-collar occupation to become less negative or perhaps positive. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "We also found that measurement error in earnings appears to pose more significant problems during recessionary periods than expansionary periods, probably because errors in reported earnings are based largely on misperceptions of hours worked. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Moreover, the standard errors in the earnings equations typically were increased by the multiple imputation procedure, because the procedure correctly accounts for sampling error that is ignored in analyses that treat earnings as an error-free variable. ",
            "section": "Multiple Imputations"
        },
        {
            "text": "Clearly, multiple imputation can make a substantial difference in both qualitative conclusions and statistical accuracy.",
            "section": "Multiple Imputations"
        },
        {
            "text": "Grand claims sometimes have been made for bootstrap analysis. ",
            "section": "Conclusions"
        },
        {
            "text": "For instance, Efron and Tibshirani (1993) and Vinod (1998) envision the bootstrap as part of a strategy to find universally applicable methods for estimation and inference, which can be implemented with very little effort or analysis by researchers. ",
            "section": "Conclusions",
            "ref_spans": [
                {
                    "start": 14,
                    "end": 41,
                    "type": "bibr",
                    "text": "Efron and Tibshirani (1993)"
                },
                {
                    "start": 46,
                    "end": 58,
                    "type": "bibr",
                    "ref_id": "b15",
                    "text": "Vinod (1998)"
                }
            ]
        },
        {
            "text": "This vision is tempting, especially given the ease and speed with which bootstrap estimates for many models can be obtained using modern desktop computers. ",
            "section": "Conclusions"
        },
        {
            "text": "However, Manski (1996) argues that this vision is flawed due to the inherent ambiguity of statistical theory in comparing alternative estimation procedures. ",
            "section": "Conclusions",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 22,
                    "type": "bibr",
                    "ref_id": "b10",
                    "text": "Manski (1996)"
                }
            ]
        },
        {
            "text": "Moreover, the current state of the art in bootstrap methods cannot support such universal application because the necessary evidence regarding bootstrap superiority (or even consistency) is not available for many classes of models. ",
            "section": "Conclusions"
        },
        {
            "text": "At this point, however, the bootstrap can effectively serve the less grandiose but very practical purpose of deriving better confidence bands in models characterized by deviations from a variety of assumptions commonly made in least squares analysis.",
            "section": "Conclusions"
        },
        {
            "text": "Similarly, multiple imputations is an approach that applied economists should consider to help alleviate problems caused by survey nonresponse, missing data and measurement error. ",
            "section": "Conclusions"
        },
        {
            "text": "The best way to circumvent these problems is to put more resources into reducing response biases during survey administration. ",
            "section": "Conclusions"
        },
        {
            "text": "The secondbest solutions include the collection of external validation data or incorporation of confidential survey information to enable accurate estimation of the nonresponse or error process. ",
            "section": "Conclusions"
        },
        {
            "text": "Under these second-best circumstances, the multiple imputation methods presented in this paper provide a straightforward and consistent way for researchers to adjust for missing and erroneous data in their modeling and forecasting efforts. ",
            "section": "Conclusions"
        },
        {
            "text": "Multiple imputation is like an adjustable crescent wrench-it rarely is the ideal tool for any particular job, but it works well for a wide variety of problems.",
            "section": "Conclusions"
        },
        {
            "text": "y We gratefully acknowledge the unusually thorough and helpful comments from the editors.",
            "section": "Conclusions"
        },
        {
            "text": "Much of this work was completed while the second author was visiting at the Organization for Economic Cooperation and Development (OECD) in Paris, France. ",
            "section": "Conclusions"
        },
        {
            "text": "The views expressed in this paper are solely those of the authors and do not necessarily represent the views of the Federal Reserve Bank of San Francisco, the Federal Reserve System or the OECD. ",
            "section": "Conclusions"
        },
        {
            "text": "The authors also are solely responsible for any errors or omissions.",
            "section": "Conclusions"
        }
    ]
}