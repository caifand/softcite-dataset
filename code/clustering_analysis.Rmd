---
title: "Cluster analysis of software mentions"
author: "James Howison and Caifan Du"
date: "10/12/2018"
output: html_document
---

```{r}
library(tidyverse)
library(here)
library(fuzzyjoin)
```


We suspect that the software mentions in a paper cluster together.  This would be useful to know because then we could do content analysis more quickly (because we wouldn't need to read as much of the paper in order to code mentions). We could use a "whitelist" of known package names to identify the "mention rich" parts of the papers, then code that.  But first we need to know what we might miss by making that change.

Procedure candidate:

On unseen papers:
1. Convert to fulltext
2. Search for "Seeds" taken from a whitelist of package names
3. Assign the text in a window around any seeds for content analysis (e.g., 500 words before and 500 words after, or 1 page before and 1 page after any seed found in the fulltext).

Analysis to see how this might perform:

1. Generate a whitelist of known package names as "seeds".
2. For each paper:
  - see if those seeds were mentioned, note PDF page where found.
  - query dataset for other mentions in that paper within X pages of where the seed was found.
  - recall = total found using seeds / total found in paper through manual coding (gives proportion of mentions that would have been included in the window.)
  - average recall across papers (or use a scatterplot)
3. Repeat for many possible random samples of seeds (and take the average as the expected recall value for each paper).

Repeat all that for varying window sizes (e.g., same page as seed, +/- 1 page, +/- 2 pages.)

Eventually we should get a curve showing the relationship between window size and recall.

load dataset.
```{r load_dataset}
articles <- read_csv(here("data/csv_dataset", "softcite_articles.csv"))
in_text_mentions <- read_csv(here("data/csv_dataset", "softcite_in_text_mentions.csv"))
codes_applied <- read_csv(here("data/csv_dataset", "softcite_codes_applied.csv"))
```

Normalize package names
```{r}
codes_applied <- codes_applied %>%
  mutate(norm_code_label = code_label %>% str_to_lower())
```


get all package names as 'seeds'.
Package names are in codes_applied
```{r}
whitelist <- codes_applied %>%
  filter(code == "software_name", was_code_present == "true") %>%
  select(norm_code_label) %>%
  unique
whitelist
```

select seeds
```{r}
curr_seeds <- whitelist %>%
  drop_na %>%
  sample_frac(0.05)
# all mentions that included those chosen seeds.
curr_seeds %>% left_join(codes_applied) %>%
  select(-coder) %>%
  left_join(in_text_mentions)
```

```{r}
all_software_found <- codes_applied %>%
  mutate(norm_code_label = code_label %>% str_to_lower()) %>%
  filter(code == "software_name",
         was_code_present == "true",
         ! is.na(norm_code_label)) %>%
    select(-coder) %>%
    left_join(in_text_mentions) %>%
    select(article, selection, page, code_label, norm_code_label)
all_software_found
```

create whitelist and sample from it.
```{r}
whitelist <- all_software_found %>%
  select(norm_code_label) %>%
  unique
whitelist
curr_seeds <- sample_frac(whitelist, 0.05)
curr_seeds
```


filter all_software_found to just seeds.
```{r}
seeds_with_articles <- all_software_found %>%
  drop_na(article) %>%
  filter(norm_code_label %in% curr_seeds$norm_code_label)
#join would be more efficient
```

join back to all_software_found by article, pdf_page window.



```{r}
find_mentions <- function(expand_window, seed_percent) {
 #expand_window <- 0
 #seed_percent <- 0.05

 tibble(rep_num = 1:5) %>%
    mutate(recall_count = pmap_int(
      list(rep_num, expand_window, seed_percent),
      find_mention_single_rep)) %>%
    summarize(mean_recall = mean(recall_count)) %>%
    pull(mean_recall) %>%
    pluck(1)

}
```

```{r}
find_mention_single_rep <- function(rep, expand_window, seed_percent) {
 curr_seeds <- all_software_found %>%
    select(norm_code_label) %>%
    unique %>%
    drop_na %>%
    sample_frac(seed_percent)

  seeds_with_articles <- all_software_found %>%
    drop_na(article) %>%
    filter(norm_code_label %in% curr_seeds$norm_code_label) %>%
    select(article, page) %>%
    distinct()

# expand_window 0 is same page, 1 is 1 page either size (ie 3 page window.)
  mentions_count <- seeds_with_articles %>%
    # gets all mentions that were on the same page as any
    # of our seed mentions.
    # left_join(all_software_found, by = c("article", "page"))
    # custom merge condition, since inequality joins are a pain.
    crossing(all_software_found) %>%
    filter(article == article1, abs(page - page1) < (expand_window + 1) ) %>%
    distinct(selection) %>%
    nrow()

  return(mentions_count)
}
```

```{r}
# set up experiment
highest_page <- all_software_found %>% drop_na %>% pull(page) %>% max
window_size <-  0:highest_page * 2
window_size <- 0:3
all_software_found %>%
  drop_na %>%
  group_by(article) %>%
  tally %>%
  top_n(1, n)
seed_percent <- 5:10 / 100
experiment <- crossing(window_size, seed_percent)
# add recall column using procedure above.
experiment <- experiment %>%
  mutate(recall_count = map2_dbl(window_size, seed_percent, find_mentions))
```



cross that with seeds so now we have a row for each paper/seed combination.  

search for those combinations.  could be in more than one spot in a paper.
paper,seed,seed_mention,pdf_page

(or start with full dataset, then filter out any that aren't our seeds)

add window sizes (can add all here)
paper,total_mention_count,seed,seed_mention,pdf_page,window_size,window_start,window_end

query others than would have been found. from each paper's set of mentions, which have a pdf page value such that: paper == paper, and window_start <= seed_pdf_page <= window_end.

Then then have to merge the mentions found for each seed query.

Calculate recall for each paper.

Draw pretty pic.
